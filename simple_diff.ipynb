{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66fb9cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ml_stuff\\gitcodes\\Papers-in-100-Lines-of-Code\\Denoising_Diffusion_Probabilistic_Models\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from unet import UNet\n",
    "from keras.datasets.mnist import load_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "658f6f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I would first need access to the samples\n",
    "\n",
    "(trainX, trainy), (testX, testy) = load_data()\n",
    "trainX = np.float32(trainX) / 255.\n",
    "testX = np.float32(testX) / 255.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b57b4af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I would then need a neural network\n",
    "mynn = UNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6411b4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I would need to define the terminal variance and the step size\n",
    "# 0.1 for sigma_t, not really sure how to choose this\n",
    "# 0.01 for step size, not really sure how to choose this\n",
    "delta_t = 0.01\n",
    "sigma_q = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c5c5355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_0 would be a sample from the data we have\n",
    "index_0  = torch.randint(low=0,high=60_000,size=(1,))\n",
    "x_0 = trainX[index_0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3df771b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The uniform sampled time stamp\n",
    "t = torch.rand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d160076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x_t = x_0 + N(0, t * sigma_q)\n",
    "mean_0 = torch.zeros(28)\n",
    "cov_x_t = sigma_q* t * torch.eye(28)\n",
    "gaussian_noise = torch.distributions.MultivariateNormal(mean_0, cov_x_t)\n",
    "x_t  = gaussian_noise.sample((1,)) + torch.tensor(x_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "03576991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create x_{t+delta_t} = x_t + N(0, sigma_q * delta_t)\n",
    "cov_x_t_delta_t = sigma_q * delta_t * torch.eye(28)\n",
    "gaussian_noise_delta_t = torch.distributions.MultivariateNormal(mean_0, cov_x_t_delta_t)\n",
    "x_t_delta_t  = gaussian_noise_delta_t.sample((1,)) + x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6ce7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need to define a loss on this single sample now,\n",
    "# the loss is ||f_theta(x_{t+delta_t}, t+delta_t) - x_t||^2_2'\n",
    "# I really need to understand wtf is this f_theta shit now,\n",
    "# and what it do with this time stuff like t.\n",
    "# f_theta should really be a neural network.\n",
    "# so down below, I will start to build my own neuron network\n",
    "# first of all, we build a resnet block\n",
    "\n",
    "# The below ResNetBlock will contain the following structure:\n",
    "# 1. a 3 by 3 conv layer, with padding 1 stride 1, this will give an output of the same dimensionality\n",
    "# 2. a Batch norm layer\n",
    "# 3. a ReLU layer\n",
    "# 4. again a 3 by 3 conv layer with padding 1 stride 1\n",
    "# 5. a batch norm layer\n",
    "# 6. an element-wise addition of the thusfar output with the input\n",
    "# 7. a ReLu layer\n",
    "# Questions: \n",
    "# 1. what is a batch norm?\n",
    "# 2. how to incorprate the t from the diffusion?\n",
    "# 3. what is a transposed convolutional layer? (just like a convolutional layer, but this upsamples)\n",
    "\n",
    "# first is a problem with normalization, if we dont normalize our data, we might have exploding gradient\n",
    "# second is non-normalized data can significantly decrease our training speed.\n",
    "\n",
    "# So a batch normalization layer you can choose to apply wherever you want in the network\n",
    "# The first thing it does is to normalize the output of the current layer\n",
    "\n",
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, num_input, num_output):\n",
    "        super().__init__()\n",
    "        self.convo = nn.Conv2d(in_channels = 1, out_channels = 1, kernel_size = 3, stride = 1, padding= 1)\n",
    "        self.batchnorm = nn.BatchNorm2d(1)\n",
    "        self.activation = nn.Softmax()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        conv_output = self.convo(inputs)\n",
    "        batchnorm = self.batchnorm(conv_output)\n",
    "        output = self.activation(batchnorm)\n",
    "        return output\n",
    "\n",
    "myres = ResNetBlock(784, 784)\n",
    "\n",
    "tensored_x0 =x_0 = torch.tensor(x_0)\n",
    "x_0 = x_0.unsqueeze(0).unsqueeze(0)\n",
    "#myres.forward(x_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fed8454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dingf\\AppData\\Local\\Temp\\ipykernel_30188\\883875903.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  output = self.activation(batchnorm)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "           1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netres = myres.forward(x_0)\n",
    "netres"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
