{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9604e129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyo/mlstuff/finee_tune/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch.nn.init as init\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e6439a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyo/mlstuff/finee_tune/venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-15): 16 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"https_proxy\"] = \"http://127.0.0.1:7890/\"\n",
    "hf_token = \"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\"\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,  padding_side = \"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype = torch.bfloat16, device_map = device, use_auth_token = hf_token)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "037c1447",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ = (\"Hi this is my first test!\")\n",
    "inputs = tokenizer(text_, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee43038f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,  13347,    420,    374,    856,   1176,   1296,      0]],\n",
       "       device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e817ac61",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_embed = model.get_input_embeddings()\n",
    "input_vectors = input_embed(inputs[\"input_ids\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7de459f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: so in torch, each word is a row.\n",
    "# NOTE: in_dim counts how many features per example, i.e. the number of columns\n",
    "# NOTEï¼š out_dim counts how many features per example, i.e. the number of columns in the output.\n",
    "class Nin(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        # NOTE: couple of notes here, the nn.Parameter tells the model to update the parameter \n",
    "        # during gradient descent\n",
    "        self.W = nn.Parameter(torch.rand(in_dim, out_dim))\n",
    "        init.xavier_uniform(self.W)\n",
    "        # NOTE: biases are not typicall xavier inited.\n",
    "        self.b = nn.Parameter(torch.zeros(out_dim))\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        # NOTE: below b stands for batch, h stands for how many words, w stands for input_dim\n",
    "        # NOTE: so its gonne ba word_size * embedding_dim(self.in_dim) for input\n",
    "        # NOTE: the self.W is inited to embedding_dim(in_dim) * output_dim\n",
    "        # NOTE: which returns a word_size * output_dim matrix\n",
    "        # NOTE self.b will be brocasted as output[b, h, o] += bias[o] along the last axis.\n",
    "        output = torch.einsum(\"bhw, wo -> bho\", input, self.W) \n",
    "        return output\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867b7dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.K_W = Nin(in_dim, out_dim)\n",
    "        self.Q_W = Nin(in_dim, out_dim)\n",
    "        self.V_W = Nin(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        K = self.K_W(input)\n",
    "        Q = self.Q_W(input)\n",
    "        V = self.V_W(input)\n",
    "\n",
    "        similarity_matrix = torch.softmax( torch.einsum(\"bij, bkj -> bik\", K, Q), dim=-1)\n",
    "        \n",
    "        \n",
    "        return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dace2ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0027,  0.0031, -0.0068,  ...,  0.0011,  0.0008,  0.0015],\n",
      "         [-0.0173,  0.0327,  0.0117,  ...,  0.0154, -0.0248,  0.0415],\n",
      "         [-0.0042, -0.0027,  0.0352,  ...,  0.0183, -0.0223, -0.0205],\n",
      "         ...,\n",
      "         [ 0.0282, -0.0060, -0.0275,  ...,  0.0037, -0.0047,  0.0170],\n",
      "         [ 0.0031,  0.0109,  0.0305,  ...,  0.0439,  0.0271, -0.0195],\n",
      "         [ 0.0031,  0.0178,  0.0210,  ..., -0.0052, -0.0420, -0.0334]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(input_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "661fd9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_71820/3723756114.py:12: FutureWarning: `nn.init.xavier_uniform` is now deprecated in favor of `nn.init.xavier_uniform_`.\n",
      "  init.xavier_uniform(self.W)\n"
     ]
    }
   ],
   "source": [
    "myattention = attention(in_dim= 2048, out_dim=512)\n",
    "myattention.to(device)\n",
    "myattention.to(torch.bfloat16)\n",
    "output = myattention(input_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6d93b4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.1270, 0.1230, 0.1250, 0.1245, 0.1270, 0.1240, 0.1250, 0.1250],\n",
       "         [0.1260, 0.1235, 0.1250, 0.1235, 0.1260, 0.1250, 0.1260, 0.1245],\n",
       "         [0.1250, 0.1270, 0.1240, 0.1260, 0.1260, 0.1250, 0.1250, 0.1230],\n",
       "         [0.1240, 0.1245, 0.1245, 0.1250, 0.1260, 0.1260, 0.1260, 0.1240],\n",
       "         [0.1245, 0.1260, 0.1260, 0.1260, 0.1250, 0.1240, 0.1250, 0.1235],\n",
       "         [0.1250, 0.1245, 0.1240, 0.1270, 0.1260, 0.1270, 0.1230, 0.1235],\n",
       "         [0.1211, 0.1250, 0.1240, 0.1250, 0.1279, 0.1260, 0.1260, 0.1250],\n",
       "         [0.1240, 0.1250, 0.1245, 0.1250, 0.1270, 0.1250, 0.1235, 0.1250]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output[0] - torch.transpose(output[0],8,8)\n",
    "torch.softmax(output, dim=-1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
